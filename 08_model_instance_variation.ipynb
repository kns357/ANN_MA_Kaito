{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd940b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b31379bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, h0, h1, out_size = 784, 32, 16, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41ca5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.MNIST(root='./data', train=False,\n",
    "                    transform=transforms.ToTensor(), download=True)\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=1000, shuffle=False)\n",
    "images, _ = next(iter(loader))\n",
    "images = images.view(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17ba112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size0, hidden_size1, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(input_size, hidden_size0)     #784 - 32\n",
    "        self.fc1 = nn.Linear(hidden_size0, hidden_size1)   #32 - 16\n",
    "        self.fc2 = nn.Linear(hidden_size1, out_size)       #16 - 10\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc0.weight, gain=nn.init.calculate_gain('tanh')) \n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        a0 = self.tanh(self.fc0(x))\n",
    "        a1 = self.tanh(self.fc1(a0))\n",
    "        out = self.fc2(a1)\n",
    "        return a0, a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "752852cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc0): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc1): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netA = Net(input_size, h0, h1, out_size)\n",
    "netA.load_state_dict(torch.load('stored_model_weights/model_inst_0'))\n",
    "netA.eval()\n",
    "netB = Net(input_size, h0, h1, out_size)\n",
    "netB.load_state_dict(torch.load('stored_model_weights/model_inst_4'))\n",
    "netB.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e56260d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting activations\n",
    "actsA0, actsA1, actsB0, actsB1 = [], [], [], []\n",
    "for img in images:\n",
    "    a0, a1 = netA(img.unsqueeze(0))\n",
    "    b0, b1 = netB(img.unsqueeze(0))\n",
    "    actsA0.append(a0.detach().numpy().flatten())\n",
    "    actsA1.append(a1.detach().numpy().flatten())\n",
    "    actsB0.append(b0.detach().numpy().flatten())\n",
    "    actsB1.append(b1.detach().numpy().flatten())\n",
    "actsA0 = np.stack(actsA0); actsA1 = np.stack(actsA1)\n",
    "actsB0 = np.stack(actsB0); actsB1 = np.stack(actsB1)\n",
    "\n",
    "#colecting similarities\n",
    "raw_cos0, raw_corr0, sorted_cos0, sorted_corr0 = [], [], [], []\n",
    "raw_cos1, raw_corr1, sorted_cos1, sorted_corr1 = [], [], [], []\n",
    "\n",
    "for a0, b0, a1, b1 in zip(actsA0, actsB0, actsA1, actsB1):\n",
    "    #raw comparison of distribution\n",
    "    raw_cos0.append(F.cosine_similarity(torch.tensor(a0), torch.tensor(b0), dim=0).item())\n",
    "    raw_corr0.append(pearsonr(a0, b0)[0])\n",
    "    raw_cos1.append(F.cosine_similarity(torch.tensor(a1), torch.tensor(b1), dim=0).item())\n",
    "    raw_corr1.append(pearsonr(a1, b1)[0])\n",
    "\n",
    "    #sorted comparison of values\n",
    "    sa0 = np.sort(a0)[::-1].copy()\n",
    "    sb0 = np.sort(b0)[::-1].copy()\n",
    "    sorted_cos0.append(F.cosine_similarity(torch.tensor(sa0), torch.tensor(sb0), dim=0).item())\n",
    "    sorted_corr0.append(pearsonr(sa0, sb0)[0])\n",
    "\n",
    "    sa1 = np.sort(a1)[::-1].copy()\n",
    "    sb1 = np.sort(b1)[::-1].copy()\n",
    "    sorted_cos1.append(F.cosine_similarity(torch.tensor(sa1), torch.tensor(sb1), dim=0).item())\n",
    "    sorted_corr1.append(pearsonr(sa1, sb1)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e56da4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 raw cosine similarity: 0.066 ± 0.156\n",
      "Layer 0 sorted cosine similarity: 0.967 ± 0.031\n",
      "Layer 0 raw Pearson correlation: -0.014 ± 0.171\n",
      "Layer 0 sorted Pearson correlation: 0.973 ± 0.024\n",
      "\n",
      "Layer 1 raw cosine similarity: 0.121 ± 0.201\n",
      "Layer 1 sorted cosine similarity: 0.931 ± 0.059\n",
      "Layer 1 raw Pearson correlation: 0.034 ± 0.195\n",
      "Layer 1 sorted Pearson correlation: 0.948 ± 0.039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report(layer, raw_cos, raw_cor, sorted_cos, sorted_cor):\n",
    "    print(f\"Layer {layer} raw cosine similarity: {np.mean(raw_cos):.3f} ± {np.std(raw_cos):.3f}\")\n",
    "    print(f\"Layer {layer} sorted cosine similarity: {np.mean(sorted_cos):.3f} ± {np.std(sorted_cos):.3f}\")\n",
    "    print(f\"Layer {layer} raw Pearson correlation: {np.mean(raw_cor):.3f} ± {np.std(raw_cor):.3f}\")\n",
    "    print(f\"Layer {layer} sorted Pearson correlation: {np.mean(sorted_cor):.3f} ± {np.std(sorted_cor):.3f}\\n\")\n",
    "\n",
    "report(0, raw_cos0, raw_corr0, sorted_cos0, sorted_corr0)\n",
    "report(1, raw_cos1, raw_corr1, sorted_cos1, sorted_corr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47982b",
   "metadata": {},
   "source": [
    "-> output will always have highly similar activation patterns, assuming all model instances were trained accurately.\n",
    "This results from only the output being explcitly optimised for, while the hidden layer only implcitly have to optimise, but aren't used for loss calculation -> their structure can be arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1cf77",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "When comparing the distribution of neuron activations across model instances for hidden layers, similarity is very low because it is arbitrarily based on the random weight init.\n",
    "When comparing the sorted neuron activations the similarity clearly peaks, suggesting that the same values are encoded, meaning the features encoded in any model instance is lilely very similar.\n",
    "Only the distribution differs, because the model does not optimise for any spatial structure (like convolutional kernels in CNNs would), but simply finds some configuration, regardless of spatial structure, that encodes the necessary features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fde98",
   "metadata": {},
   "source": [
    "In the next experiment 09_subnet_encoding.ipynb I will look for such spatialy ignorant but feature-aware encoding, not for the entire digit, but for lower-level concepts that humans can interpret, unlike the pixels that neurons encode normally as seen in 04_neuron_attention(_binary).ipynb.\n",
    "Having seen the scattered neuron attention in experiments 04, I expect not to find any lower-level concepts clearly encoded in sub-networks.\n",
    "\n",
    "The model loses information about which human and low-level concepts actually compose the pixels a neuron \"looks at\". That is because they don't optimise for recognising low-level concepts, instead they just need some way of encoding the entire digit, for which they can use any arbitrary pixel combionation summing up to a useful activation in the forward pass, ignorant of human-interpretable concepts. For the simplicity of the experiments I define a human-interpretable conept as a localised and continuously connected cluster of pixels, rather than the scattered attention that my neurons seem to display.\n",
    "\n",
    "This is why their attention (which will be formally defined in the paper) may be scattered and still achieve high accuracy on the given OCR tasks (as well as tasks from outside of CV)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
